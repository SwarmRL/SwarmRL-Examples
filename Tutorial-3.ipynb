{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: Simple Reinforcement Learning\n",
    "\n",
    "This tutorial aims to show you how to implement simple reinforcement learning agents into an ESPResSo simulation. After the tutorial, you will have learned:\n",
    "\n",
    "* How to implement a reinforcement learning simulation in SwarmRL\n",
    "* How to save a re-load trained agents\n",
    "* Some tips for doing reinforcement learning with SwarmRL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Once again, we must import some packages to get us through the tutorial.  We will try to keep the imports minimal and use longer calls during the tutorial so you know where a module or class comes from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SwarmRL Imports\n",
    "import swarmrl as srl\n",
    "\n",
    "# ESPResSo Imports\n",
    "import espressomd\n",
    "\n",
    "# Linalg Imports\n",
    "import numpy as np\n",
    "\n",
    "# Neural Network Imports\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "\n",
    "# Unit Handling\n",
    "import pint\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Simulation\n",
    "\n",
    "Now, we need to create a simulation box and add colloids. We will add 40 colloids of two types to a 1000-micrometer box. We will also move to a slightly different interface for building simulations, namely one in which the system runner will be called from a function. The reason will become apparent in later tutorials, but it is good to start early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ureg = pint.UnitRegistry()  # Still define this outside.\n",
    "\n",
    "system = espressomd.System(box_l=[1, 2, 3])  # This is just a dummy holder.\n",
    "def get_system_runner(system):\n",
    "    \"\"\"\n",
    "    Create a system runner.\n",
    "    \"\"\"\n",
    "    md_params = srl.engine.espresso.MDParams(\n",
    "            ureg=ureg,\n",
    "            fluid_dyn_viscosity=ureg.Quantity(8.9e-4, \"pascal * second\"),\n",
    "            WCA_epsilon=ureg.Quantity(293, \"kelvin\") * ureg.boltzmann_constant,\n",
    "            temperature=ureg.Quantity(293, \"kelvin\"),\n",
    "            box_length=ureg.Quantity(3 * [1000], \"micrometer\"),\n",
    "            time_slice=ureg.Quantity(0.2, \"second\"),  # model timestep\n",
    "            time_step=ureg.Quantity(0.02, \"second\") / 5,  # integrator timestep\n",
    "            write_interval=ureg.Quantity(2, \"second\"),\n",
    "        )\n",
    "    system_runner = srl.engine.espresso.EspressoMD(\n",
    "            md_params=md_params,\n",
    "            n_dims=2,\n",
    "            seed=np.random.randint(5453),  # seed for the simulation velocities\n",
    "            out_folder=\"tutorial-2\",\n",
    "            write_chunk_size=1000,  # Used for dumping to the database.\n",
    "            system=system,  # Add the pre-defined system.\n",
    "        )\n",
    "    \n",
    "    # Add type 0 colloids to the simulation\n",
    "    system_runner.add_colloids(\n",
    "            n_colloids=20,  # Let's make 10 of them\n",
    "            radius_colloid=ureg.Quantity(1.0, \"micrometer\"),\n",
    "            random_placement_center=ureg.Quantity(\n",
    "                np.array([500, 500, 0]), \"micrometer\"\n",
    "            ),\n",
    "            random_placement_radius=ureg.Quantity(60, \"micrometer\"),\n",
    "            type_colloid=0,  # These are type 0\n",
    "        )\n",
    "    \n",
    "    # Add type 1 colloids to the simulation\n",
    "    system_runner.add_colloids(\n",
    "            n_colloids=20,  # Let's make 10 of them\n",
    "            radius_colloid=ureg.Quantity(1.0, \"micrometer\"),\n",
    "            random_placement_center=ureg.Quantity(\n",
    "                np.array([500, 500, 0]), \"micrometer\"\n",
    "            ),\n",
    "            random_placement_radius=ureg.Quantity(60, \"micrometer\"),\n",
    "            type_colloid=1,  # These are type 1\n",
    "        )\n",
    "    \n",
    "    return system_runner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Reinforcement Learning Agent\n",
    "\n",
    "Now, we can create an RL agent that can learn to perform chemotaxis. To do so, we need to define the following properties:\n",
    "\n",
    "* Observable: What the agent sees in its environment.\n",
    "* Task: What we want the agent to achieve.\n",
    "* Actions: What the agent is capable of doing.\n",
    "* Network: The neural networks used in the agent's brain.\n",
    "\n",
    "In this tutorial, we will use chemical sensing as the observable. This means the agent can sense some chemical or field changes around it when moving. As a task, we will ask the agent to maximise these changes to move closer to the source of the chemical. The actions will be simple: translation along its directional axis, rotation clockwise, counterclockwise, or doing nothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decay Function\n",
    "For both the task and the observable, we need to define how the concentration of our field falls off from its source. Let's use a simple linear function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_fn(distance: np.ndarray):\n",
    "    return 1.0 - distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observable\n",
    "Now, we can define the observable. The agent will receive this as input to the neural network when it makes decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observable = srl.observables.ConcentrationField(\n",
    "        source=np.array([500.0, 500.0, 0.0]),  # Source is the middle of the box\n",
    "        decay_fn=decay_fn,\n",
    "        scale_factor=1000,  # Scales the reward which might otherwise be very small.\n",
    "        box_length=np.array([1000.0, 1000.0, 1000]),  # Normalizes distances.\n",
    "        particle_type=0,  # Only acts on type 0 colloids.\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "Let's now give our colloids something to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = srl.tasks.searching.GradientSensing(\n",
    "        source=np.array([500.0, 500.0, 0.0]),\n",
    "        decay_function=decay_fn,\n",
    "        reward_scale_factor=1000,\n",
    "        box_length=np.array([1000.0, 1000.0, 1000]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions\n",
    "Now, some actions. Note that the keys in the following dictionary are just for your benefit; the agent cares about the position of the action, not its name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = {\n",
    "    \"RotateClockwise\": srl.actions.Action(torque=np.array([0.0, 0.0, 10.0])),\n",
    "    \"Translate\": srl.actions.Action(force=10.0),\n",
    "    \"RotateCounterClockwise\": srl.actions.Action(torque=np.array([0.0, 0.0, 10.0])),\n",
    "    \"DoNothing\": srl.actions.Action(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brain\n",
    "Now, let's give our agent a brain. The architecture of SwarmRL is such that we take a single neural network module that can have as many networks inside it as desired. In this case, we will use a shared architecture for the neural network. This means that the actor-networkand critic networks will share layers.\n",
    "\n",
    "We should make a point here that we are leaving out some optional parameters that can be added to the network: an exploration policy and a sampling strategy. These will be discussed in later tutorials, which go into more detail about what one can change about the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNet(nn.Module):\n",
    "    \"\"\"A simple dense neural newtork.\"\"\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(features=12)(x)  # Shared layer,\n",
    "        x = nn.relu(x)\n",
    "        y = nn.Dense(features=1)(x)\n",
    "        x = nn.Dense(features=4)(x)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now convert this into a SwarmRL network.\n",
    "network = srl.networks.FlaxModel(\n",
    "    flax_model=ActorCriticNet(),\n",
    "    optimizer=optax.adam(learning_rate=0.001),\n",
    "    input_shape=(1,)  # Ipnut to the network is a single number.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent\n",
    "We can finally define an agent that will be controlled by this neural network and trained during the simulations. Today, we are using the Actor-critical agent, which, as the name suggests, is prepared using actor-critic reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = srl.agents.ActorCriticAgent(\n",
    "    particle_type=0,\n",
    "    network=network,\n",
    "    task=task,\n",
    "    observable=observable,\n",
    "    actions=actions,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer\n",
    "Now, we need to decide how to train the network. There are a few ways to do this:\n",
    "* Episodic: Run the agents for a set amount before resetting the simulation and updating the networks.\n",
    "* Semi-episodic: Update the agents during the simulation but reset it occasionally.\n",
    "* Continuous: Start the simulation and let it run while the agents try to learn.\n",
    "\n",
    "In this tutorial, we will try out both approaches together and as a demonstration of a realistic training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_trainer = srl.trainers.ContinuousTrainer(\n",
    "    [agent],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Now, we can train the neural network. To do so, we run 100 episodes of length 50. This 50 means that the network is called for new actions 50 times inside the simulation before all rewards are collected and used to update the network. After training, we can plot the rewards to see how the agents are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "\n",
    "system_runner = get_system_runner(system)\n",
    "rewards.append(continuous_trainer.perform_rl_training(\n",
    "    system_runner=system_runner,\n",
    "    n_episodes=500,\n",
    "    episode_length=10,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.concatenate((rewards))\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the agents haven't started to learn. But training is still early on. Try increasing the number of episodes and see if you notice when the agents begin to train.\n",
    "\n",
    "Let's say you have trained for a while with continuous training, but the agents are running away from their target or are unable to find good rewards. Let's try to train them with the episodic trainer for a while and see if it makes a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodic_trainer = srl.trainers.EpisodicTrainer(\n",
    "    [agent],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards.append(episodic_trainer.perform_rl_training(\n",
    "    get_engine=get_system_runner,\n",
    "    n_episodes=500,\n",
    "    system=system,\n",
    "    episode_length=50,  # We should give them more time to reach the target.\n",
    "    reset_frequency=1  # Increase this for semi-episodic training\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.concatenate(rewards))\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe they're doing a bit better, but they take a while to reach the centre, and I am unsure if they receive enough feedback. This is an excellent point to test semi-episodic training to get the best of both worlds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_episodic_trainer = srl.trainers.EpisodicTrainer(\n",
    "    [agent],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards.append(semi_episodic_trainer.perform_rl_training(\n",
    "    get_engine=get_system_runner,\n",
    "    n_episodes=500,\n",
    "    system=system,\n",
    "    episode_length=10,\n",
    "    reset_frequency=20  # Reset the environment after 20 episodes.\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.concatenate(rewards))\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, I can't promise that any of this will work. RL sometimes converges quickly, and other times not at all. However, with enough training, this should find its source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the agent\n",
    "Training is one thing, but we want to be able to deploy this agent into simulations without training or even continue training this agent later on. To do so, we can save the state of the agent networks and optimizer either through the trainer or the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.export_models(directory=\"Models\") #<-- Alternative method.\n",
    "agent.save_agent(\"Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to, we could load this agent up again and keep training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.restore_models(directory=\"Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, I can also create the agent again, load in the state of it, and deploy it directly in my simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = srl.agents.ActorCriticAgent(\n",
    "    particle_type=0,\n",
    "    network=network,\n",
    "    task=task,\n",
    "    observable=observable,\n",
    "    actions=actions,\n",
    ")\n",
    "agent.restore_agent(\"Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I will create a force function using the agent and run an ESPResSo simulation with my newly trained agents. Keep in mind no training will occur now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_fn = srl.force_functions.ForceFunction({\"0\": agent})\n",
    "system_runner = get_system_runner(system)\n",
    "system_runner.integrate(1000, force_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
